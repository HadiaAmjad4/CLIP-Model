# CLIP-Model

**Author:** Hadia AMJAD – 22415258  
**Course:** Multi-modalité et IA générative, Master 2 – VMI – UP Cité  
**GitHub:** [CLIP-Model](https://github.com/yourusername/CLIP-Model)  

---

## Overview
This project explores **CLIP (Contrastive Language–Image Pretraining)**, a model that jointly embeds images and text. The repository demonstrates CLIP on natural and medical images, zero-shot classification, and compares different model variants.

---

## Repository Structure

CLIP-Model/
├── CLIP_notebook.ipynb      # Main notebook with all experiments
├── Report_CLIP.pdf          # Project report
└── README.md                # This file

---
